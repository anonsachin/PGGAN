{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataExtraction(params):\n",
    "#     transform = transforms.Compose([\n",
    "#     transforms.CenterCrop(160),\n",
    "#     transforms.ToTensor()])\n",
    "#     img = datasets.ImageFolder(root='asl_alphabets/gan_train/',\n",
    "#                                transform=transform)\n",
    "#     dataload = torch.utils.data.DataLoader(img,**params)\n",
    "#     return dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 16,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(160),\n",
    "    transforms.ToTensor()])\n",
    "data = datasets.FashionMNIST(root = 'data',train=True,download=True,\n",
    "                                           transform=transforms.ToTensor())\n",
    "loader = torch.utils.data.DataLoader(data,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faf7c2d82b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,_= next(iter(loader))\n",
    "x = x.numpy()\n",
    "img = np.squeeze(x[0])\n",
    "\n",
    "fig = plt.figure(figsize = (3,3)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "# plt.imshow(np.transpose(x[0].numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pixelNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pixelNormalization,self).__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        eps = 1e-10\n",
    "        batch, dim = x.shape[0], x.shape[1]\n",
    "        y,z = x.shape[2] , x.shape[3]\n",
    "        x = x.view(batch,dim,-1)\n",
    "        mean = x.mean(1)\n",
    "        dev = x.var(1)\n",
    "        mean = mean.view(batch,1,-1)\n",
    "        dev = dev.view(batch,1,-1)\n",
    "        x = (x - mean)/(dev.sqrt() + eps)\n",
    "        x = x.view(batch,dim,y,z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tconv_section(inp,out,*args,**kargs):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(inp,out,*args,**kargs),\n",
    "#         pixelNormalization(),\n",
    "         nn.BatchNorm2d(out),\n",
    "        nn.LeakyReLU(negative_slope=0.1)\n",
    "    )\n",
    "def tconv_block(a,*args,**kargs):\n",
    "    layers = [tconv_section(i,j,*args,**kargs) for i,j in zip(a,a[1:])]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): ConvTranspose2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv_block([1,16,16], kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(in_size,*args,**kargs):\n",
    "    out =1\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_size,out,*args,**kargs),\n",
    "#         pixelNormalization(),\n",
    "#          nn.BatchNorm2d(out),\n",
    "        nn.LeakyReLU(negative_slope=0.1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights\n",
    "def init_weights(x,mean=0,std=1):\n",
    "    i = x.__class__.__name__\n",
    "    if (i == 'Conv2d' or i == 'Linear' or i == 'ConvTranspose2d'):\n",
    "        nn.init.normal_(x.weight.data, mean=mean, std=std)\n",
    "\n",
    "# var = 2/fan_in version of initialization - https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528\n",
    "def he_initialization(x):\n",
    "    i = x.__class__.__name__\n",
    "    if (i == 'Conv2d' or i == 'ConvTranspose2d'):\n",
    "        x.weight*(math.sqrt(2/(x.weight.shape[1]*x.weight.shape[2]*x.weight.shape[3])))\n",
    "    elif (i == 'Linear'):\n",
    "        x.weight*(math.sqrt(2/x.weight.shape[1]))\n",
    "        \n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    x = x*(feature_range[1] - feature_range[0]) + feature_range[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_size=100):\n",
    "        super(Generator,self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(z_size,49),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dict = nn.ModuleDict({\n",
    "            'layer_1': tconv_block([1,16,16], kernel_size=3, padding=1),\n",
    "            'Upsample': nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            'to_rgb_1': to_rgb(16,kernel_size=3, padding=1)\n",
    "        })\n",
    "    def grow(self,n,args):\n",
    "        self.dict['layer_{}'.format(n)] = tconv_block(args, kernel_size=3, padding=1)\n",
    "        self.dict['to_rgb_{}'.format(n)] = to_rgb(args[-1], kernel_size=3, padding=1)\n",
    "        self.dict['layer_{}'.format(n)].apply(init_weights)\n",
    "        self.dict['to_rgb_{}'.format(n)].apply(init_weights)\n",
    "    \n",
    "    def train_g(self,x,n,aplha):\n",
    "        self.dict.apply(he_initialization)\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.linear(x)\n",
    "        x = x.view(batch_size,1,7,7)\n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                x = self.dict['layer_{}'.format(i+1)](x)\n",
    "#                 x = pixelNormalization(x)\n",
    "                if n == 1:\n",
    "                    x = self.dict['to_rgb_{}'.format(i+1)](x)\n",
    "            elif i+1 == n:\n",
    "                x = self.dict['Upsample'](x)\n",
    "                out = self.dict['layer_{}'.format(i+1)](x)\n",
    "                out = self.dict['to_rgb_{}'.format(i+1)](out)\n",
    "#                 out = pixelNormalization(out)\n",
    "                x = self.dict['to_rgb_{}'.format(i)](x)\n",
    "#                 x = pixelNormalization(x)\n",
    "                x = (1-aplha)*x + aplha*(out)\n",
    "            else:\n",
    "                x = self.dict['Upsample'](x)\n",
    "                x = self.dict['layer_{}'.format(i+1)](x)\n",
    "#                 x = pixelNormalization(x)\n",
    "            x = torch.tanh(x)\n",
    "        return x\n",
    "                \n",
    "    def forward(self,x,n):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.linear(x)\n",
    "        x = x.view(batch_size,1,7,7)\n",
    "        \n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                x = self.dict['layer_{}'.format(i+1)](x)\n",
    "            else:\n",
    "                x = self.dict['Upsample'](x)\n",
    "                x = self.dict['layer_{}'.format(i+1)](x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_section(inp,out,*args,**kargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp,out,*args,**kargs),\n",
    "#         pixelNormalization(),\n",
    "        nn.BatchNorm2d(out),\n",
    "        nn.LeakyReLU(negative_slope=0.1)\n",
    "    )\n",
    "def conv_block(a,*args,**kargs):\n",
    "    layers = [conv_section(i,j,*args,**kargs) for i,j in zip(a,a[1:])]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def from_rgb(out,*args,**kargs):\n",
    "    in_size = 1\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_size,out,*args,**kargs),\n",
    "#         pixelNormalization(),\n",
    "        nn.BatchNorm2d(out),\n",
    "        nn.LeakyReLU(negative_slope=0.1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.down = nn.AvgPool2d(kernel_size=2)\n",
    "        self.dict = nn.ModuleDict({\n",
    "            'layer_1': conv_block([(16+1),16,1], kernel_size=3, padding=1),# account for minbatch stddev\n",
    "            'Upsample': nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            'from_rgb_1': from_rgb(16,kernel_size=3, padding=1)\n",
    "        })\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(49,1)\n",
    "#             nn.Linear(10,1)\n",
    "        )\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def grow(self,n,args):\n",
    "        rgb = args[0] \n",
    "        self.dict['layer_{}'.format(n)] = conv_block(args, kernel_size=3, padding=1)\n",
    "        self.dict['from_rgb_{}'.format(n)] = from_rgb(rgb, kernel_size=3, padding=1)\n",
    "        self.dict['layer_{}'.format(n)].apply(init_weights)\n",
    "        self.dict['from_rgb_{}'.format(n)].apply(init_weights)\n",
    "        \n",
    "    def train_d(self,x,n,alpha):\n",
    "        x = scale(x)\n",
    "        self.dict.apply(he_initialization)\n",
    "        for i in range(n,0,-1):\n",
    "            if n == 1:\n",
    "                x = self.dict['from_rgb_{}'.format(n)](x)\n",
    "                x = self.min_batch_stddev(x)\n",
    "                x = self.dict['layer_{}'.format(n)](x)\n",
    "            elif i == n:\n",
    "                store = self.dict['from_rgb_{}'.format(n-1)](self.down(x))\n",
    "                x = self.dict['from_rgb_{}'.format(n)](x)\n",
    "                x = self.dict['layer_{}'.format(n)](x)\n",
    "                x = self.down(x)\n",
    "            elif i == n-1:\n",
    "                x = (1-alpha)*x + alpha*(store)\n",
    "                if i == 1:\n",
    "                    x = self.min_batch_stddev(x)\n",
    "                x = self.dict['layer_{}'.format(i)](x)\n",
    "            else:\n",
    "                if i == 1:\n",
    "                    x = self.min_batch_stddev(x)\n",
    "                x = self.dict['layer_{}'.format(i)](x)\n",
    "                x = self.down(x)\n",
    "        \n",
    "        x = x.view(-1,49)\n",
    "        x = self.linear(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def downSample(self,x,n):\n",
    "        if n < 0:\n",
    "            return x\n",
    "        for i in range(n):\n",
    "            x = self.down(x)\n",
    "            \n",
    "        return x\n",
    "    def min_batch_stddev(self,x):\n",
    "#         min batch standard deviation pixel-wise std\n",
    "        y,z = x.shape[2],x.shape[3]\n",
    "        batch = x.shape[0]\n",
    "        x = x.view(batch,x.shape[1],-1)\n",
    "        dev = x.std(1)\n",
    "        dev = dev.view(batch,1,-1)\n",
    "        x = torch.cat([x,dev],1)\n",
    "        x = x.view(-1,x.shape[1],y,z)\n",
    "        return x\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (down): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (dict): ModuleDict(\n",
       "    (Upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (from_rgb_1): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (layer_1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=49, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = Discriminator()\n",
    "D.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=49, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (dict): ModuleDict(\n",
       "    (Upsample): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (layer_1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (to_rgb_1): Sequential(\n",
       "      (0): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator()\n",
    "G.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def real_loss(x,smooth=False):\n",
    "#     x = torch.relu(x)\n",
    "#     if smooth == True:\n",
    "#         loss = torch.mean(torch.abs(x - 0.9))\n",
    "#     else:\n",
    "#         loss = torch.mean(torch.abs(x - 1))\n",
    "        \n",
    "#     return loss\n",
    "\n",
    "# def fake_loss(x):\n",
    "#     x = torch.relu(x)\n",
    "#     loss = torch.mean(torch.abs(x - 0.1))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out,smooth = False):\n",
    "    batch_size = D_out.size(0)\n",
    "    if smooth:\n",
    "        labels =torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size)\n",
    "        \n",
    "    if torch.cuda.is_available:\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    loss = crit(D_out.squeeze(),labels)\n",
    "    return loss\n",
    "def fake_loss(D_out):\n",
    "    labels = torch.ones(D_out.size(0))*0.1\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    if torch.cuda.is_available:\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    loss = crit(D_out.squeeze(),labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth dimmensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dim = [[16,16,16],[16,16,16]]\n",
    "d_dim = [[16,16,16],[16,16,16]]\n",
    "# down sampling times\n",
    "down_times = [2,1,0]\n",
    "# epochs \n",
    "epochs = [25,25,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the 1 level\n",
      "Epoch/Phase :0/1 : The Discriminator loss = 0.8848 || The Generator loss = 1.7800\n",
      "Epoch/Phase :1/1 : The Discriminator loss = 0.7077 || The Generator loss = 2.0935\n",
      "Epoch/Phase :2/1 : The Discriminator loss = 0.8073 || The Generator loss = 2.1390\n",
      "Epoch/Phase :3/1 : The Discriminator loss = 0.7101 || The Generator loss = 2.8657\n",
      "Epoch/Phase :4/1 : The Discriminator loss = 0.8130 || The Generator loss = 2.1206\n",
      "Epoch/Phase :5/1 : The Discriminator loss = 0.7194 || The Generator loss = 2.4941\n",
      "Epoch/Phase :6/1 : The Discriminator loss = 0.7209 || The Generator loss = 2.5987\n",
      "Epoch/Phase :7/1 : The Discriminator loss = 0.7021 || The Generator loss = 3.2598\n",
      "Epoch/Phase :8/1 : The Discriminator loss = 0.7512 || The Generator loss = 2.1608\n",
      "Epoch/Phase :9/1 : The Discriminator loss = 0.7070 || The Generator loss = 2.4109\n",
      "Epoch/Phase :10/1 : The Discriminator loss = 0.7307 || The Generator loss = 2.0301\n",
      "Epoch/Phase :11/1 : The Discriminator loss = 0.8342 || The Generator loss = 1.8645\n",
      "Epoch/Phase :12/1 : The Discriminator loss = 0.7717 || The Generator loss = 2.0328\n",
      "Epoch/Phase :13/1 : The Discriminator loss = 0.7385 || The Generator loss = 1.6917\n",
      "Epoch/Phase :14/1 : The Discriminator loss = 0.7879 || The Generator loss = 0.9820\n",
      "Epoch/Phase :15/1 : The Discriminator loss = 0.7688 || The Generator loss = 1.4066\n",
      "Epoch/Phase :16/1 : The Discriminator loss = 0.8271 || The Generator loss = 1.8771\n",
      "Epoch/Phase :17/1 : The Discriminator loss = 0.8983 || The Generator loss = 1.7304\n",
      "Epoch/Phase :18/1 : The Discriminator loss = 0.8054 || The Generator loss = 1.1223\n",
      "Epoch/Phase :19/1 : The Discriminator loss = 0.8403 || The Generator loss = 1.6080\n",
      "Epoch/Phase :20/1 : The Discriminator loss = 0.8126 || The Generator loss = 1.7185\n",
      "Epoch/Phase :21/1 : The Discriminator loss = 1.0837 || The Generator loss = 1.5235\n",
      "Epoch/Phase :22/1 : The Discriminator loss = 0.8281 || The Generator loss = 2.7756\n",
      "Epoch/Phase :23/1 : The Discriminator loss = 0.7920 || The Generator loss = 1.6618\n",
      "Epoch/Phase :24/1 : The Discriminator loss = 1.0517 || The Generator loss = 1.3348\n",
      "Starting the 2 level\n",
      "Epoch/Phase :0/2 : The Discriminator loss = 1.0538 || The Generator loss = 1.1423\n",
      "Epoch/Phase :1/2 : The Discriminator loss = 0.8037 || The Generator loss = 1.4664\n",
      "Epoch/Phase :2/2 : The Discriminator loss = 0.7970 || The Generator loss = 2.4105\n",
      "Epoch/Phase :3/2 : The Discriminator loss = 0.9265 || The Generator loss = 2.6397\n",
      "Epoch/Phase :4/2 : The Discriminator loss = 0.7555 || The Generator loss = 1.2822\n",
      "Epoch/Phase :5/2 : The Discriminator loss = 0.7109 || The Generator loss = 2.0372\n",
      "Epoch/Phase :6/2 : The Discriminator loss = 0.6669 || The Generator loss = 2.1764\n",
      "Epoch/Phase :7/2 : The Discriminator loss = 0.6579 || The Generator loss = 2.2922\n",
      "Epoch/Phase :8/2 : The Discriminator loss = 0.6545 || The Generator loss = 2.3300\n",
      "Epoch/Phase :9/2 : The Discriminator loss = 0.6528 || The Generator loss = 2.2908\n",
      "Epoch/Phase :10/2 : The Discriminator loss = 0.6514 || The Generator loss = 2.2862\n",
      "Epoch/Phase :11/2 : The Discriminator loss = 0.6521 || The Generator loss = 2.3088\n",
      "Epoch/Phase :12/2 : The Discriminator loss = 0.6545 || The Generator loss = 2.1782\n",
      "Epoch/Phase :13/2 : The Discriminator loss = 0.6538 || The Generator loss = 2.2964\n",
      "Epoch/Phase :14/2 : The Discriminator loss = 0.6572 || The Generator loss = 2.2322\n",
      "Epoch/Phase :15/2 : The Discriminator loss = 0.6625 || The Generator loss = 1.8291\n",
      "Epoch/Phase :16/2 : The Discriminator loss = 0.6611 || The Generator loss = 2.2333\n",
      "Epoch/Phase :17/2 : The Discriminator loss = 0.6539 || The Generator loss = 2.2780\n",
      "Epoch/Phase :18/2 : The Discriminator loss = 0.6533 || The Generator loss = 2.3105\n",
      "Epoch/Phase :19/2 : The Discriminator loss = 0.6530 || The Generator loss = 2.3331\n",
      "Epoch/Phase :20/2 : The Discriminator loss = 0.6522 || The Generator loss = 2.2950\n",
      "Epoch/Phase :21/2 : The Discriminator loss = 0.6524 || The Generator loss = 2.2569\n",
      "Epoch/Phase :22/2 : The Discriminator loss = 0.7223 || The Generator loss = 2.8116\n",
      "Epoch/Phase :23/2 : The Discriminator loss = 0.6526 || The Generator loss = 2.2936\n",
      "Epoch/Phase :24/2 : The Discriminator loss = 0.6632 || The Generator loss = 2.2286\n",
      "Starting the 3 level\n",
      "Epoch/Phase :0/3 : The Discriminator loss = 0.6616 || The Generator loss = 2.1575\n",
      "Epoch/Phase :1/3 : The Discriminator loss = 0.6511 || The Generator loss = 2.3025\n",
      "Epoch/Phase :2/3 : The Discriminator loss = 0.7186 || The Generator loss = 1.9675\n",
      "Epoch/Phase :3/3 : The Discriminator loss = 0.6604 || The Generator loss = 2.2488\n",
      "Epoch/Phase :4/3 : The Discriminator loss = 0.6732 || The Generator loss = 2.2954\n",
      "Epoch/Phase :5/3 : The Discriminator loss = 0.7000 || The Generator loss = 2.7530\n",
      "Epoch/Phase :6/3 : The Discriminator loss = 0.6530 || The Generator loss = 2.2573\n",
      "Epoch/Phase :7/3 : The Discriminator loss = 0.6513 || The Generator loss = 2.2855\n",
      "Epoch/Phase :8/3 : The Discriminator loss = 0.6512 || The Generator loss = 2.2922\n",
      "Epoch/Phase :9/3 : The Discriminator loss = 0.6504 || The Generator loss = 2.3602\n",
      "Epoch/Phase :10/3 : The Discriminator loss = 0.6635 || The Generator loss = 2.7015\n",
      "Epoch/Phase :11/3 : The Discriminator loss = 0.6546 || The Generator loss = 2.3017\n",
      "Epoch/Phase :12/3 : The Discriminator loss = 0.6536 || The Generator loss = 2.4163\n",
      "Epoch/Phase :13/3 : The Discriminator loss = 0.6545 || The Generator loss = 2.3220\n",
      "Epoch/Phase :14/3 : The Discriminator loss = 0.6528 || The Generator loss = 2.3034\n",
      "Epoch/Phase :15/3 : The Discriminator loss = 0.6660 || The Generator loss = 2.5465\n",
      "Epoch/Phase :16/3 : The Discriminator loss = 0.6523 || The Generator loss = 2.2855\n",
      "Epoch/Phase :17/3 : The Discriminator loss = 0.6540 || The Generator loss = 2.3042\n",
      "Epoch/Phase :18/3 : The Discriminator loss = 0.6525 || The Generator loss = 2.2821\n",
      "Epoch/Phase :19/3 : The Discriminator loss = 0.6610 || The Generator loss = 2.2862\n",
      "Epoch/Phase :20/3 : The Discriminator loss = 0.6569 || The Generator loss = 2.2826\n",
      "Epoch/Phase :21/3 : The Discriminator loss = 0.6570 || The Generator loss = 2.5359\n",
      "Epoch/Phase :22/3 : The Discriminator loss = 0.6553 || The Generator loss = 2.2818\n",
      "Epoch/Phase :23/3 : The Discriminator loss = 0.6763 || The Generator loss = 2.3145\n",
      "Epoch/Phase :24/3 : The Discriminator loss = 0.6949 || The Generator loss = 1.5401\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0002\n",
    "beta1=0.3\n",
    "beta2=0.999  \n",
    "test = np.random.normal(0,1,size=(params['batch_size'],100)) #torch.randn(16,100)\n",
    "test = torch.from_numpy(test).float()\n",
    "losses =[]\n",
    "j =0 \n",
    "for i in range(1,4):\n",
    "    print('Starting the {} level'.format(i))\n",
    "    if i != 1:\n",
    "        D.grow(i,d_dim[j])\n",
    "        G.grow(i,g_dim[j])\n",
    "        j +=1\n",
    "    # Create optimizers for the discriminator D and generator G\n",
    "    d_optimizer = optim.Adam(D.parameters(),lr=lr,betas=[beta1, beta2])\n",
    "    g_optimizer = optim.Adam(G.parameters(),lr=lr*beta2,betas=[beta1, beta2])\n",
    "    if torch.cuda.is_available:\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "    for e in range(epochs[i-1]):\n",
    "        alpha = (e+1)/epochs[i-1]\n",
    "        for x,_ in loader:\n",
    "            if torch.cuda.is_available:\n",
    "                x = x.cuda()\n",
    "            d_optimizer.zero_grad()\n",
    "            # real\n",
    "            x = D.downSample(x,down_times[i-1])\n",
    "            out_real = D.train_d(x,i,alpha)\n",
    "            d_real = real_loss(out_real,smooth=True)\n",
    "            # fake\n",
    "            z = np.random.normal(0,1,size=(params['batch_size'],100))\n",
    "            z= torch.from_numpy(z).float()\n",
    "            if torch.cuda.is_available:\n",
    "                z = z.cuda()\n",
    "            out_fake = G.train_g(z,i,alpha)\n",
    "            out_fake = D.train_d(out_fake,i,alpha)\n",
    "            d_fake = fake_loss(out_fake)\n",
    "            \n",
    "            d_loss = d_real+d_fake\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            # generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            z = np.random.normal(0,1,size=(params['batch_size'],100))\n",
    "            z= torch.from_numpy(z).float()\n",
    "            if torch.cuda.is_available:\n",
    "                z = z.cuda()\n",
    "            out_g = G.train_g(z,i,alpha)\n",
    "            out_g = D.train_d(out_g,i,alpha)\n",
    "            g_loss = real_loss(out_g)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "        else:\n",
    "            if e %1 == 0:\n",
    "                losses.append((d_loss.item(),g_loss.item()))\n",
    "                print('Epoch/Phase :{}/{} : The Discriminator loss = {:6.4f} || The Generator loss = {:6.4f}'.format(e,i,d_loss.item(),g_loss.item()))\n",
    "#     else:\n",
    "#         G.eval()\n",
    "#         if torch.cuda.is_available:\n",
    "#             test = test.cuda()\n",
    "#         img = G.train_g(test,i,1)\n",
    "#         G.train()\n",
    "#         img = img.cpu().detach()\n",
    "#         plt.imshow(np.transpose(img[0].numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faf780d8898>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "G.eval()\n",
    "if torch.cuda.is_available:\n",
    "        test = test.cuda()\n",
    "img = G.train_g(test,3,1)\n",
    "G.train()\n",
    "img = img.cpu().detach().numpy()\n",
    "img = np.squeeze(img[2])\n",
    "\n",
    "fig = plt.figure(figsize = (3,3)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "# plt.imshow(np.transpose(img[0].numpy(), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(img[2])\n",
    "\n",
    "fig = plt.figure(figsize = (3,3)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[10].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
